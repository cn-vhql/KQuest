"""Streamlitå¯è§†åŒ–ç•Œé¢æ¨¡å—"""

import streamlit as st
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from kquest.config import get_config, Config
from kquest.extractor import KnowledgeExtractor
from kquest.storage import KnowledgeStorage
from kquest.models import KnowledgeGraph, KnowledgeTriple, QueryResult
from kquest.reasoning import KnowledgeReasoner

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def init_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if 'config' not in st.session_state:
        st.session_state.config = get_config()
    if 'extractor' not in st.session_state:
        st.session_state.extractor = None
    if 'reasoner' not in st.session_state:
        st.session_state.reasoner = None
    if 'knowledge_graph' not in st.session_state:
        st.session_state.knowledge_graph = None
    if 'extraction_result' not in st.session_state:
        st.session_state.extraction_result = None
    if 'query_history' not in st.session_state:
        st.session_state.query_history = []

def init_services():
    """åˆå§‹åŒ–æœåŠ¡"""
    try:
        if not st.session_state.config.openai.api_key:
            st.error("è¯·åœ¨é…ç½®é¡µé¢è®¾ç½®OpenAI API Key")
            return False

        st.session_state.extractor = KnowledgeExtractor(st.session_state.config)
        st.session_state.reasoner = KnowledgeReasoner(st.session_state.config)
        return True
    except Exception as e:
        st.error(f"åˆå§‹åŒ–æœåŠ¡å¤±è´¥: {str(e)}")
        return False

def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    st.sidebar.title("KQuest çŸ¥è¯†å›¾è°±ç³»ç»Ÿ")

    page = st.sidebar.selectbox(
        "é€‰æ‹©åŠŸèƒ½é¡µé¢",
        ["é…ç½®ç®¡ç†", "çŸ¥è¯†æŠ½å–", "æ™ºèƒ½é—®ç­”", "å›¾è°±å¯è§†åŒ–", "ç³»ç»Ÿä¿¡æ¯"]
    )

    # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
    if st.session_state.config.openai.api_key:
        st.sidebar.success("âœ“ API Keyå·²é…ç½®")
    else:
        st.sidebar.warning("âš ï¸ éœ€è¦é…ç½®API Key")

    if st.session_state.knowledge_graph:
        st.sidebar.info(f"ğŸ“Š å½“å‰å›¾è°±: {len(st.session_state.knowledge_graph.triples)} ä¸ªä¸‰å…ƒç»„")

    return page

def render_config_page():
    """æ¸²æŸ“é…ç½®ç®¡ç†é¡µé¢"""
    st.header("âš™ï¸ é…ç½®ç®¡ç†")

    # OpenAIé…ç½®
    st.subheader("OpenAI API é…ç½®")

    with st.form("openai_config"):
        api_key = st.text_input(
            "API Key",
            type="password",
            value=st.session_state.config.openai.api_key or "",
            help="OpenAI API Key"
        )

        base_url = st.text_input(
            "Base URL (å¯é€‰)",
            value=st.session_state.config.openai.base_url or "",
            help="è‡ªå®šä¹‰APIç«¯ç‚¹ï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤"
        )

        model = st.selectbox(
            "æ¨¡å‹é€‰æ‹©",
            options=["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"],
            index=0 if st.session_state.config.openai.model == "gpt-3.5-turbo" else 0
        )

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=2.0,
            value=st.session_state.config.openai.temperature,
            step=0.1
        )

        max_tokens = st.number_input(
            "Max Tokens",
            min_value=100,
            max_value=8000,
            value=st.session_state.config.openai.max_tokens or 2000
        )

        submitted = st.form_submit_button("ä¿å­˜é…ç½®")

        if submitted:
            if not api_key:
                st.error("API Keyä¸èƒ½ä¸ºç©º")
                return

            # æ›´æ–°é…ç½®
            st.session_state.config.openai.update_api_key(api_key)
            st.session_state.config.openai.base_url = base_url if base_url else None
            st.session_state.config.openai.model = model
            st.session_state.config.openai.temperature = temperature
            st.session_state.config.openai.max_tokens = max_tokens

            # é‡æ–°åˆå§‹åŒ–æœåŠ¡
            if init_services():
                st.success("é…ç½®ä¿å­˜æˆåŠŸï¼")
            else:
                st.error("é…ç½®ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®")

    # æŠ½å–é…ç½®
    st.subheader("çŸ¥è¯†æŠ½å–é…ç½®")

    with st.form("extraction_config"):
        chunk_size = st.number_input(
            "æ–‡æ¡£åˆ†å—å¤§å°",
            min_value=500,
            max_value=10000,
            value=st.session_state.config.extraction.chunk_size,
            step=100
        )

        chunk_overlap = st.number_input(
            "åˆ†å—é‡å å¤§å°",
            min_value=0,
            max_value=1000,
            value=st.session_state.config.extraction.chunk_overlap,
            step=50
        )

        confidence_threshold = st.slider(
            "ç½®ä¿¡åº¦é˜ˆå€¼",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.config.extraction.min_confidence,
            step=0.1
        )

        submitted = st.form_submit_button("æ›´æ–°æŠ½å–é…ç½®")

        if submitted:
            st.session_state.config.extraction.chunk_size = chunk_size
            st.session_state.config.extraction.chunk_overlap = chunk_overlap
            st.session_state.config.extraction.min_confidence = confidence_threshold
            st.success("æŠ½å–é…ç½®æ›´æ–°æˆåŠŸï¼")

def render_extraction_page():
    """æ¸²æŸ“çŸ¥è¯†æŠ½å–é¡µé¢"""
    st.header("ğŸ“„ çŸ¥è¯†æŠ½å–")

    if not st.session_state.extractor:
        st.error("è¯·å…ˆåœ¨é…ç½®é¡µé¢è®¾ç½®API Key")
        return

    # æ–‡ä»¶ä¸Šä¼ 
    st.subheader("ä¸Šä¼ æ–‡æ¡£")
    uploaded_file = st.file_uploader(
        "é€‰æ‹©è¦æŠ½å–çš„æ–‡æ¡£",
        type=['txt', 'md', 'json'],
        help="æ”¯æŒTXTã€Markdownã€JSONæ ¼å¼"
    )

    if uploaded_file:
        st.info(f"å·²é€‰æ‹©æ–‡ä»¶: {uploaded_file.name}")

        # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹é¢„è§ˆ
        content = uploaded_file.read().decode('utf-8')
        with st.expander("æ–‡ä»¶å†…å®¹é¢„è§ˆ"):
            st.text_area("å†…å®¹", content, height=200)

        # æŠ½å–å‚æ•°
        st.subheader("æŠ½å–å‚æ•°")
        st.info("æŠ½å–å°†ä½¿ç”¨é…ç½®é¡µé¢ä¸­çš„å‚æ•°è®¾ç½®")

        # å¼€å§‹æŠ½å–
        if st.button("ğŸš€ å¼€å§‹æŠ½å–", type="primary"):
            with st.spinner("æ­£åœ¨æŠ½å–çŸ¥è¯†å›¾è°±..."):
                try:
                    # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
                    uploaded_file.seek(0)
                    temp_file_path = f"/tmp/{uploaded_file.name}"

                    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                    with open(temp_file_path, 'w', encoding='utf-8') as f:
                        f.write(content)

                    # æ‰§è¡ŒæŠ½å–
                    result = st.session_state.extractor.extract_from_file_sync(
                        temp_file_path
                    )

                    if result.success:
                        st.session_state.knowledge_graph = result.knowledge_graph
                        st.session_state.extraction_result = result

                        st.success(f"æŠ½å–å®Œæˆï¼æˆåŠŸæŠ½å– {result.extracted_triples} ä¸ªä¸‰å…ƒç»„")

                        # æ˜¾ç¤ºæŠ½å–ç»“æœ
                        render_extraction_result(result)
                    else:
                        st.error(f"æŠ½å–å¤±è´¥: {result.error_message}")

                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.remove(temp_file_path)

                except Exception as e:
                    st.error(f"æŠ½å–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                    logger.error(f"Extraction error: {e}")

def render_extraction_result(result):
    """æ¸²æŸ“æŠ½å–ç»“æœ"""
    st.subheader("ğŸ“Š æŠ½å–ç»“æœ")

    # ç»Ÿè®¡ä¿¡æ¯
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ä¸‰å…ƒç»„æ•°é‡", result.extracted_triples)

    with col2:
        st.metric("å¤„ç†æ—¶é—´", f"{result.processing_time:.2f}s")

    with col3:
        stats = result.knowledge_graph.get_statistics()
        st.metric("å”¯ä¸€å®ä½“", stats['unique_subjects'] + stats['unique_objects'])

    with col4:
        st.metric("å…³ç³»ç±»å‹", stats['unique_predicates'])

    # ä¸‰å…ƒç»„åˆ—è¡¨
    st.subheader("æŠ½å–çš„ä¸‰å…ƒç»„")

    # è¿‡æ»¤é€‰é¡¹
    col1, col2 = st.columns(2)
    with col1:
        filter_type = st.selectbox(
            "è¿‡æ»¤ç±»å‹",
            ["å…¨éƒ¨", "é«˜ç½®ä¿¡åº¦", "ä¸­ç½®ä¿¡åº¦", "ä½ç½®ä¿¡åº¦"]
        )
    with col2:
        search_term = st.text_input("æœç´¢å…³é”®è¯")

    # è¿‡æ»¤å’Œæœç´¢ä¸‰å…ƒç»„
    filtered_triples = []
    for triple in result.knowledge_graph.triples:
        # ç½®ä¿¡åº¦è¿‡æ»¤
        if filter_type == "é«˜ç½®ä¿¡åº¦" and triple.confidence < 0.8:
            continue
        elif filter_type == "ä¸­ç½®ä¿¡åº¦" and not (0.5 <= triple.confidence < 0.8):
            continue
        elif filter_type == "ä½ç½®ä¿¡åº¦" and triple.confidence >= 0.5:
            continue

        # å…³é”®è¯æœç´¢
        if search_term:
            if (search_term.lower() not in triple.subject.lower() and
                search_term.lower() not in triple.predicate.lower() and
                search_term.lower() not in triple.object.lower()):
                continue

        filtered_triples.append(triple)

    # æ˜¾ç¤ºä¸‰å…ƒç»„
    if filtered_triples:
        for i, triple in enumerate(filtered_triples):
            with st.expander(f"{triple.subject} --{triple.predicate}--> {triple.object} (ç½®ä¿¡åº¦: {triple.confidence:.2f})"):
                col1, col2 = st.columns(2)
                with col1:
                    st.write(f"**ç±»å‹**: {triple.triple_type.value}")
                    st.write(f"**ç½®ä¿¡åº¦**: {triple.confidence:.2f}")
                    st.write(f"**ç½®ä¿¡åº¦çº§åˆ«**: {triple.confidence_level.value}")
                with col2:
                    if triple.source:
                        st.write(f"**æ¥æº**: {triple.source[:100]}...")
                    if triple.metadata:
                        st.write("**å…ƒæ•°æ®**:")
                        st.json(triple.metadata)
    else:
        st.info("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ä¸‰å…ƒç»„")

def render_qa_page():
    """æ¸²æŸ“æ™ºèƒ½é—®ç­”é¡µé¢"""
    st.header("ğŸ¤– æ™ºèƒ½é—®ç­”")

    if not st.session_state.knowledge_graph:
        st.warning("è¯·å…ˆåœ¨çŸ¥è¯†æŠ½å–é¡µé¢åˆ›å»ºçŸ¥è¯†å›¾è°±")
        return

    if not st.session_state.reasoner:
        st.error("è¯·å…ˆåœ¨é…ç½®é¡µé¢è®¾ç½®API Key")
        return

    # é—®ç­”è¾“å…¥
    st.subheader("æå‡ºé—®é¢˜")

    question = st.text_input(
        "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
        placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        key="question_input"
    )

    # æ¨ç†æ¨¡å¼é€‰æ‹©
    st.subheader("ğŸ§  æ¨ç†æ¨¡å¼")
    col1, col2, col3 = st.columns(3)

    reasoning_modes = {
        "graph": {
            "name": "çº¯å›¾ç®—æ³•",
            "description": "åŸºäºä¼ ç»Ÿå›¾ç®—æ³•çš„å¿«é€Ÿæ¨ç†",
            "icon": "ğŸ“Š",
            "speed": "æå¿«",
            "quality": "åŸºç¡€"
        },
        "hybrid": {
            "name": "æ··åˆæ¨ç†",
            "description": "ç»“åˆå›¾ç®—æ³•ä¸LLMè¯­ä¹‰ç†è§£",
            "icon": "ğŸ”—",
            "speed": "ä¸­ç­‰",
            "quality": "é«˜"
        },
        "llm_driven": {
            "name": "LLMé©±åŠ¨",
            "description": "å¤§æ¨¡å‹ä¸ºä¸»ä½“çš„æ™ºèƒ½æ¨ç†",
            "icon": "ğŸš€",
            "speed": "è¾ƒæ…¢",
            "quality": "æœ€é«˜"
        }
    }

    selected_mode = None
    for i, (mode, info) in enumerate(reasoning_modes.items()):
        with [col1, col2, col3][i]:
            if st.button(
                f"{info['icon']} {info['name']}",
                help=f"{info['description']}\\né€Ÿåº¦: {info['speed']} | è´¨é‡: {info['quality']}",
                use_container_width=True,
                type="primary" if st.session_state.get('selected_reasoning_mode') == mode else "secondary"
            ):
                selected_mode = mode

    # å¦‚æœæ²¡æœ‰é€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æˆ–ä¹‹å‰çš„é€‰æ‹©
    if selected_mode is None:
        selected_mode = st.session_state.get('selected_reasoning_mode', 'graph')

    st.session_state.selected_reasoning_mode = selected_mode

    # æ˜¾ç¤ºå½“å‰é€‰æ‹©å’Œæ¨¡å¼å¯¹æ¯”
    current_mode_info = reasoning_modes[selected_mode]
    st.info(f"å½“å‰æ¨¡å¼: {current_mode_info['icon']} **{current_mode_info['name']}** - {current_mode_info['description']}")

    # æ¨¡å¼å¯¹æ¯”è¡¨æ ¼
    with st.expander("ğŸ“Š æ¨ç†æ¨¡å¼å¯¹æ¯”"):
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.markdown("**æ¨¡å¼**")
            for mode, info in reasoning_modes.items():
                st.markdown(f"{info['icon']} {info['name']}")

        with col2:
            st.markdown("**é€Ÿåº¦**")
            for mode, info in reasoning_modes.items():
                if info['speed'] == 'æå¿«':
                    st.markdown("âš¡ æå¿«")
                elif info['speed'] == 'ä¸­ç­‰':
                    st.markdown("ğŸš¶ ä¸­ç­‰")
                else:
                    st.markdown("ğŸŒ è¾ƒæ…¢")

        with col3:
            st.markdown("**è´¨é‡**")
            for mode, info in reasoning_modes.items():
                if info['quality'] == 'æœ€é«˜':
                    st.markdown("â­â­â­ æœ€é«˜")
                elif info['quality'] == 'é«˜':
                    st.markdown("â­â­ é«˜")
                else:
                    st.markdown("â­ åŸºç¡€")

        with col4:
            st.markdown("**é€‚ç”¨åœºæ™¯**")
            st.markdown("â€¢ å¿«é€ŸæŸ¥è¯¢")
            st.markdown("â€¢ å¹³è¡¡æ€§èƒ½")
            st.markdown("â€¢ å¤æ‚åˆ†æ")

    # åˆ›å»ºå¯¹åº”æ¨¡å¼çš„æ¨ç†å™¨
    if (st.session_state.reasoner is None or
        st.session_state.get('current_reasoning_mode') != selected_mode):

        try:
            st.session_state.reasoner = KnowledgeReasoner(
                config=st.session_state.config,
                knowledge_graph=st.session_state.knowledge_graph,
                reasoning_mode=selected_mode
            )
            st.session_state.current_reasoning_mode = selected_mode
            st.success(f"å·²åˆ‡æ¢åˆ° {current_mode_info['name']} æ¨¡å¼")
        except Exception as e:
            st.error(f"åˆ‡æ¢æ¨ç†æ¨¡å¼å¤±è´¥: {str(e)}")
            return

    if st.button("ğŸ’¡ æé—®", type="primary") and question:
        with st.spinner("æ­£åœ¨æ€è€ƒ..."):
            try:
                # æ‰§è¡Œé—®ç­”
                result = st.session_state.reasoner.query_sync(
                    question,
                    st.session_state.knowledge_graph
                )

                # æ·»åŠ æ¨ç†æ¨¡å¼ä¿¡æ¯åˆ°ç»“æœä¸­
                result.metadata['reasoning_mode'] = selected_mode
                result.metadata['reasoning_mode_name'] = current_mode_info['name']
                result.metadata['reasoning_mode_icon'] = current_mode_info['icon']

                # æ·»åŠ åˆ°å†å²è®°å½•
                st.session_state.query_history.append(result)

                # æ˜¾ç¤ºç»“æœ
                render_qa_result(result, current_mode_info)

            except Exception as e:
                st.error(f"é—®ç­”è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                logger.error(f"QA error: {e}")

    # å†å²è®°å½•
    if st.session_state.query_history:
        st.subheader("ğŸ’¬ é—®ç­”å†å²")

        for i, result in enumerate(reversed(st.session_state.query_history[-5:])):
            # è·å–æ¨ç†æ¨¡å¼ä¿¡æ¯
            mode_icon = result.metadata.get('reasoning_mode_icon', 'â“')
            mode_name = result.metadata.get('reasoning_mode_name', 'æœªçŸ¥æ¨¡å¼')

            with st.expander(f"Q: {result.question} ({mode_icon} {mode_name}, ç½®ä¿¡åº¦: {result.confidence:.2f})"):
                st.write(f"**A**: {result.answer}")

                # æ˜¾ç¤ºæ¨ç†æ–¹æ³•
                if result.metadata.get('method'):
                    st.write(f"ğŸ”§ **æ¨ç†æ–¹æ³•**: {result.metadata['method']}")

                if result.reasoning_path:
                    st.write("**æ¨ç†è¿‡ç¨‹**:")
                    for step in result.reasoning_path:
                        st.write(f"- {step}")

                if result.source_triples:
                    st.write("**æ¥æºä¸‰å…ƒç»„**:")
                    for triple in result.source_triples:
                        st.write(f"- {triple}")

def render_qa_result(result, mode_info=None):
    """æ¸²æŸ“é—®ç­”ç»“æœ"""
    # æ˜¾ç¤ºæ¨ç†æ¨¡å¼
    if mode_info and result.metadata.get('reasoning_mode'):
        st.success(f"{mode_info['icon']} ä½¿ç”¨ **{mode_info['name']}** æ¨¡å¼å›ç­”")

    st.subheader("ğŸ’¡ å›ç­”")

    col1, col2, col3 = st.columns([3, 1, 1])
    with col1:
        st.write(result.answer)
    with col2:
        st.metric("ç½®ä¿¡åº¦", f"{result.confidence:.2f}")
    with col3:
        if mode_info:
            st.metric("æ¨¡å¼", mode_info['name'])

    # æ˜¾ç¤ºæ¨ç†æ–¹æ³•ä¿¡æ¯
    if result.metadata.get('method'):
        st.info(f"ğŸ”§ æ¨ç†æ–¹æ³•: {result.metadata['method']}")

    # æ¨ç†è·¯å¾„
    if result.reasoning_path:
        st.subheader("ğŸ” æ¨ç†è¿‡ç¨‹")
        for i, step in enumerate(result.reasoning_path, 1):
            st.write(f"{i}. {step}")

    # æ¥æºä¸‰å…ƒç»„
    if result.source_triples:
        st.subheader("ğŸ“š ç›¸å…³çŸ¥è¯†")
        for triple in result.source_triples:
            st.write(f"- {triple}")

    # æ˜¾ç¤ºé¢å¤–å…ƒæ•°æ®
    extra_metadata = {k: v for k, v in result.metadata.items()
                     if k not in ['reasoning_mode', 'reasoning_mode_name', 'reasoning_mode_icon', 'method']}
    if extra_metadata:
        st.subheader("â„¹ï¸ é™„åŠ ä¿¡æ¯")
        for key, value in extra_metadata.items():
            st.write(f"- **{key}**: {value}")

def render_visualization_page():
    """æ¸²æŸ“å›¾è°±å¯è§†åŒ–é¡µé¢"""
    st.header("ğŸ¨ çŸ¥è¯†å›¾è°±å¯è§†åŒ–")

    if not st.session_state.knowledge_graph:
        st.warning("è¯·å…ˆåœ¨çŸ¥è¯†æŠ½å–é¡µé¢åˆ›å»ºçŸ¥è¯†å›¾è°±")
        return

    kg = st.session_state.knowledge_graph

    # å›¾è°±ç»Ÿè®¡
    st.subheader("ğŸ“Š å›¾è°±ç»Ÿè®¡")
    stats = kg.get_statistics()

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ä¸‰å…ƒç»„æ€»æ•°", stats['total_triples'])
    with col2:
        st.metric("å”¯ä¸€ä¸»è¯­", stats['unique_subjects'])
    with col3:
        st.metric("å”¯ä¸€å®¾è¯­", stats['unique_objects'])
    with col4:
        st.metric("å”¯ä¸€å…³ç³»", stats['unique_predicates'])

    # ä¸‰å…ƒç»„ç±»å‹åˆ†å¸ƒ
    st.subheader("ğŸ“ˆ ä¸‰å…ƒç»„ç±»å‹åˆ†å¸ƒ")

    try:
        import matplotlib.pyplot as plt
        import pandas as pd

        # ä¸‰å…ƒç»„ç±»å‹åˆ†å¸ƒé¥¼å›¾
        triple_types = stats['triple_types']
        if any(triple_types.values()):
            col1, col2 = st.columns(2)

            with col1:
                st.write("**ä¸‰å…ƒç»„ç±»å‹åˆ†å¸ƒ**")
                # åˆ›å»ºé¥¼å›¾
                fig1, ax1 = plt.subplots(figsize=(6, 4))
                labels = [k for k, v in triple_types.items() if v > 0]
                sizes = [v for k, v in triple_types.items() if v > 0]
                ax1.pie(sizes, labels=labels, autopct='%1.1f%%')
                ax1.set_title('ä¸‰å…ƒç»„ç±»å‹åˆ†å¸ƒ')
                st.pyplot(fig1)
                plt.close()

            with col2:
                st.write("**ç½®ä¿¡åº¦åˆ†å¸ƒ**")
                # åˆ›å»ºæŸ±çŠ¶å›¾
                fig2, ax2 = plt.subplots(figsize=(6, 4))
                confidence_dist = stats['confidence_distribution']
                levels = list(confidence_dist.keys())
                counts = list(confidence_dist.values())
                ax2.bar(levels, counts)
                ax2.set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
                ax2.set_ylabel('æ•°é‡')
                st.pyplot(fig2)
                plt.close()
        else:
            st.info("æš‚æ— æ•°æ®")
    except ImportError:
        st.warning("å¯è§†åŒ–åŠŸèƒ½éœ€è¦å®‰è£… matplotlib åº“ã€‚è¯·è¿è¡Œ: pip install matplotlib")
        # æ˜¾ç¤ºç®€å•çš„æ–‡æœ¬ç»Ÿè®¡
        st.write("**ä¸‰å…ƒç»„ç±»å‹åˆ†å¸ƒ**:")
        for triple_type, count in stats['triple_types'].items():
            if count > 0:
                st.write(f"- {triple_type}: {count}")

        st.write("**ç½®ä¿¡åº¦åˆ†å¸ƒ**:")
        for level, count in stats['confidence_distribution'].items():
            if count > 0:
                st.write(f"- {level}: {count}")

    # å®ä½“å…³ç³»ç½‘ç»œ
    st.subheader("ğŸ•¸ï¸ å®ä½“å…³ç³»ç½‘ç»œ")

    try:
        # åˆ›å»ºç®€å•çš„ç½‘ç»œå›¾
        import networkx as nx
        import matplotlib.pyplot as plt

        G = nx.DiGraph()

        # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
        for triple in kg.triples[:50]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡ä»¥æé«˜æ€§èƒ½
            G.add_edge(triple.subject, triple.object, label=triple.predicate)

        if G.number_of_nodes() > 0:
            fig, ax = plt.subplots(figsize=(12, 8))
            pos = nx.spring_layout(G, k=1, iterations=50)

            # ç»˜åˆ¶èŠ‚ç‚¹
            nx.draw_networkx_nodes(G, pos, node_color='lightblue',
                                  node_size=500, alpha=0.7, ax=ax)

            # ç»˜åˆ¶è¾¹
            nx.draw_networkx_edges(G, pos, edge_color='gray',
                                  arrows=True, arrowsize=20, alpha=0.5, ax=ax)

            # ç»˜åˆ¶æ ‡ç­¾
            nx.draw_networkx_labels(G, pos, font_size=8, ax=ax)

            ax.set_title("çŸ¥è¯†å›¾è°±ç½‘ç»œç»“æ„ (æ˜¾ç¤ºå‰50ä¸ªä¸‰å…ƒç»„)")
            ax.axis('off')
            st.pyplot(fig)
            plt.close()
        else:
            st.info("å›¾è°±ä¸ºç©º")
    except ImportError:
        st.warning("ç½‘ç»œå¯è§†åŒ–åŠŸèƒ½éœ€è¦å®‰è£… networkx å’Œ matplotlib åº“ã€‚")
        # æ˜¾ç¤ºç®€å•çš„æ–‡æœ¬å…³ç³»
        st.write("**å®ä½“å…³ç³»**:")
        for i, triple in enumerate(kg.triples[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            st.write(f"{i+1}. {triple.subject} --{triple.predicate}--> {triple.object}")
        if len(kg.triples) > 10:
            st.write(f"... è¿˜æœ‰ {len(kg.triples) - 10} ä¸ªä¸‰å…ƒç»„")

    # å…³ç³»åˆ—è¡¨
    st.subheader("ğŸ”— å…³ç³»åˆ—è¡¨")

    # æŒ‰å…³ç³»ç»Ÿè®¡
    predicate_counts = {}
    for triple in kg.triples:
        predicate_counts[triple.predicate] = predicate_counts.get(triple.predicate, 0) + 1

    if predicate_counts:
        try:
            import pandas as pd
            pred_df = pd.DataFrame([
                {'å…³ç³»': pred, 'æ•°é‡': count}
                for pred, count in sorted(predicate_counts.items(), key=lambda x: x[1], reverse=True)
            ])
            st.dataframe(pred_df, use_container_width=True)
        except ImportError:
            st.warning("æ•°æ®è¡¨æ ¼åŠŸèƒ½éœ€è¦å®‰è£… pandas åº“ã€‚")
            # æ˜¾ç¤ºç®€å•çš„æ–‡æœ¬åˆ—è¡¨
            st.write("**å…³ç³»ç»Ÿè®¡**:")
            for pred, count in sorted(predicate_counts.items(), key=lambda x: x[1], reverse=True):
                st.write(f"- {pred}: {count}")
    else:
        st.info("æš‚æ— å…³ç³»æ•°æ®")

def render_info_page():
    """æ¸²æŸ“ç³»ç»Ÿä¿¡æ¯é¡µé¢"""
    st.header("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")

    # ç³»ç»Ÿé…ç½®
    st.subheader("âš™ï¸ å½“å‰é…ç½®")

    col1, col2 = st.columns(2)

    with col1:
        st.write("**OpenAI é…ç½®**")
        st.write(f"- æ¨¡å‹: {st.session_state.config.openai.model}")
        st.write(f"- Temperature: {st.session_state.config.openai.temperature}")
        st.write(f"- Max Tokens: {st.session_state.config.openai.max_tokens}")
        if st.session_state.config.openai.base_url:
            st.write(f"- Base URL: {st.session_state.config.openai.base_url}")

    with col2:
        st.write("**æŠ½å–é…ç½®**")
        st.write(f"- åˆ†å—å¤§å°: {st.session_state.config.extraction.chunk_size}")
        st.write(f"- åˆ†å—é‡å : {st.session_state.config.extraction.chunk_overlap}")
        st.write(f"- ç½®ä¿¡åº¦é˜ˆå€¼: {st.session_state.config.extraction.min_confidence}")

    # å›¾è°±ä¿¡æ¯
    if st.session_state.knowledge_graph:
        st.subheader("ğŸ“Š å½“å‰å›¾è°±ä¿¡æ¯")

        stats = st.session_state.knowledge_graph.get_statistics()

        col1, col2 = st.columns(2)

        with col1:
            st.metric("ä¸‰å…ƒç»„æ€»æ•°", stats['total_triples'])
            st.metric("åˆ›å»ºæ—¶é—´", st.session_state.knowledge_graph.created_at.strftime('%Y-%m-%d %H:%M:%S'))

        with col2:
            st.metric("æ›´æ–°æ—¶é—´", st.session_state.knowledge_graph.updated_at.strftime('%Y-%m-%d %H:%M:%S'))
            st.metric("å…ƒæ•°æ®å­—æ®µ", len(st.session_state.knowledge_graph.metadata))

    # ä½¿ç”¨è¯´æ˜
    st.subheader("ğŸ“– ä½¿ç”¨è¯´æ˜")

    st.markdown("""
    ### KQuest çŸ¥è¯†å›¾è°±ç³»ç»Ÿä½¿ç”¨æŒ‡å—

    1. **é…ç½®ç®¡ç†**: è®¾ç½®OpenAI API Keyå’Œç›¸å…³å‚æ•°
    2. **çŸ¥è¯†æŠ½å–**: ä¸Šä¼ æ–‡æ¡£ï¼Œè‡ªåŠ¨æŠ½å–çŸ¥è¯†ä¸‰å…ƒç»„
    3. **æ™ºèƒ½é—®ç­”**: åŸºäºæŠ½å–çš„çŸ¥è¯†å›¾è°±è¿›è¡Œé—®ç­”
    4. **å›¾è°±å¯è§†åŒ–**: æŸ¥çœ‹çŸ¥è¯†å›¾è°±çš„ç»Ÿè®¡ä¿¡æ¯å’Œç½‘ç»œç»“æ„

    ### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
    - TXT: çº¯æ–‡æœ¬æ–‡ä»¶
    - MD: Markdownæ ¼å¼æ–‡ä»¶
    - JSON: JSONæ ¼å¼æ–‡ä»¶

    ### é—®ç­”ç¤ºä¾‹
    - "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    - "Xå’ŒYæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
    - "åˆ—å‡ºæ‰€æœ‰çš„Zå±æ€§"
    """)

def main():
    """ä¸»å‡½æ•°"""
    st.set_page_config(
        page_title="KQuest çŸ¥è¯†å›¾è°±ç³»ç»Ÿ",
        page_icon="ğŸ§ ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    init_session_state()

    # åˆå§‹åŒ–æœåŠ¡
    if not st.session_state.extractor and st.session_state.config.openai.api_key:
        init_services()

    # æ¸²æŸ“ä¾§è¾¹æ 
    page = render_sidebar()

    # æ ¹æ®é€‰æ‹©æ¸²æŸ“å¯¹åº”é¡µé¢
    if page == "é…ç½®ç®¡ç†":
        render_config_page()
    elif page == "çŸ¥è¯†æŠ½å–":
        render_extraction_page()
    elif page == "æ™ºèƒ½é—®ç­”":
        render_qa_page()
    elif page == "å›¾è°±å¯è§†åŒ–":
        render_visualization_page()
    elif page == "ç³»ç»Ÿä¿¡æ¯":
        render_info_page()

if __name__ == "__main__":
    main()